name: Database Backup

on:
  schedule:
    - cron: '0 5 * * 0' # Runs every Sunday at 5:00 AM UTC
  workflow_dispatch:

jobs:
  db-backup:
    runs-on: ubuntu-latest

    env:
      RETENTION: 60
      DATABASE_URL: ${{ secrets.DATABASE_URL }}
      PG_VERSION: '17'
      CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
      R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
      R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
      R2_BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
      R2_REGION: 'auto' # Cloudflare R2 typically uses 'auto'
      R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }} # e.g. https://<accountid>.r2.cloudflarestorage.com

    steps:
      - name: Install PostgreSQL
        run: |
          sudo apt update
          yes '' | sudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh
          sudo apt install -y postgresql-${{ env.PG_VERSION }}

      - name: Set PostgreSQL binary path
        run:
          echo "POSTGRES=/usr/lib/postgresql/${{ env.PG_VERSION }}/bin" >>
          $GITHUB_ENV

      - name: Configure AWS CLI for Cloudflare R2
        run: |
          echo "AWS_ACCESS_KEY_ID=${{ env.R2_ACCESS_KEY_ID }}" >> $GITHUB_ENV
          echo "AWS_SECRET_ACCESS_KEY=${{ env.R2_SECRET_ACCESS_KEY }}" >> $GITHUB_ENV
          echo "AWS_DEFAULT_REGION=${{ env.R2_REGION }}" >> $GITHUB_ENV
          echo "AWS_EC2_METADATA_DISABLED=true" >> $GITHUB_ENV

      - name: Set file, folder and path variables
        run: |
          GZIP_NAME="$(date +'%B-%d-%Y@%H:%M:%S').sql.gz"
          FOLDER_NAME="${{ github.workflow }}"
          UPLOAD_PATH="${FOLDER_NAME}/${GZIP_NAME}"

          echo "GZIP_NAME=${GZIP_NAME}" >> $GITHUB_ENV
          echo "FOLDER_NAME=${FOLDER_NAME}" >> $GITHUB_ENV
          echo "UPLOAD_PATH=${UPLOAD_PATH}" >> $GITHUB_ENV

      - name: Run pg_dump
        run: |
          $POSTGRES/pg_dump ${{ env.DATABASE_URL }} | gzip > "${{ env.GZIP_NAME }}"

      - name: Delete old files (older than retention)
        run: |
          aws s3api list-objects \
            --bucket ${{ env.R2_BUCKET_NAME }} \
            --endpoint-url ${{ env.R2_ENDPOINT }} \
            --prefix "${{ env.FOLDER_NAME }}/" \
            --query "Contents[?LastModified<'$(date -d "-${{ env.RETENTION }} days" --utc +%Y-%m-%dT%H:%M:%SZ)'].Key" \
            --output text | while read -r file; do
              [ -n "$file" ] && aws s3 rm "s3://${{ env.R2_BUCKET_NAME }}/${file}" --endpoint-url ${{ env.R2_ENDPOINT }}
            done

      - name: Upload to Cloudflare R2
        run: |
          aws s3 cp "${{ env.GZIP_NAME }}" "s3://${{ env.R2_BUCKET_NAME }}/${{ env.UPLOAD_PATH }}" \
            --endpoint-url ${{ env.R2_ENDPOINT }}
